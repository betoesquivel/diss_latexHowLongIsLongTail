

\section{Related Work} %RW
The following section explains some background on the area of Entity Linking,
and describes the context around Entity Linking applications and evaluation,
as well as current research done in the linking of Long-tail Entities.
From this point on, whenever the paper refers to a tagger, it is refering to a generic application that assigns tags to text.
This facilitates the comparison between Stanford's NER Tagger and Spotlight's DBPedia's applications by treating them both as taggers;
and enforcing the equal role they both play on the description of the Long-tail Entity set for a specific domain.

\subsection{Entity linking overview} %ELO
% Describe what entity recognition is
% -What is entity recognition
% Describe current entity linking approaches
% -What is entity linking
% -Popular entity linkers available
% Dbpedia algorithm description
% -As an example, DBPedia algorithm description
%% How is this related: It is statistical, so it generates long-tail entities.
%% How I build on this: Proper description for the long-tail entity set and measurement? 

%%%%%%%%%%
% Describe what entity recognition is
% -What is entity recognition
% -Stanford NER and Open NLP description
This section will mainly focus on describing the core concepts behind Entity Linking, the current state-of-the art implementation for solving this task, and how it struggles to classify a particular set of entities, which is the main focus of this paper, Long-tail Entities.
It will also briefly describe the task of Entity Recognition, which commonly precedes Entity Linking and is the basis for the approaches used in this paper to describe the set of Long-tail Entities.

Entity Linking is an Information Extraction (IE) process by which named entities in text are identified and linked to a
"corresponding node in a Knowledge Base"\cite{rw_elo_HACHEY2013130}. 
According to \cite{silone}, it also includes the task of clustering all NIL entitites, which are the entities the couldn't be linked to a Knowledge Base (KB).
However, this is only a secondary task. More detailed research on the subject of NIL entities and their relationship to Long-tailed entities will be shown
in section \ref{nilvslongtail}.
Entity Linking is broadening its scope as a task currently, incorporating other languages into the approach \cite{silone}, trying to solve the problem
of multi-lingual entity linking. 

The first step in this task, identifying Named Entities, belongs to a category of its own, Named Entity Recognition (NER).
\cite{rw_elo_Nadeau2009} best describes the purpose of this task as the identification of entities with "rigid designators",
such as proper names and terms. Work in this area classifies Entities mostly using the collection of types known as "enamex": persons, locations and organizations \cite{rw_elo_Nadeau2009},
; the most commonly used methods to do this been statistical approaches.
Two popular examples of statistical approaches to NER been the software packages: Stanford Named Entity Recognizer\cite{rw_elo_finkel2005} and Apache OpenNLP\cite{rw_elo_kottmann2011};
both of which play an important role in this paper. Stanford NER is used in this research as the default tagger for identifying Named Entities.

On the other hand, Apache OpenNLP is used in the underlying algorithm for "spotting" candidate Named Entities to link to Wikipedia by DBPedia Spotlight\cite{rw_elo_isem2013daiber},
which is amongst the state-of-the-art methods for performing entity linking. \cite{rw_elo_isem2013daiber}, like other approaches to Entity Linking, outputs a set of entitites or "spots" tagged with, either a link to an entry in DBPedia\cite{rw_elo_morsey2012dbpedia}, or the "nil" tag \cite{rw_elo_HACHEY2013130}. 

%% describe dbpedias algorithm

\subsubsection{Current problems in EL at the moment}
The statistical nature of the algorithm, in addition to the usage of DBPedia as a Knowledge Base, means that uncommon mentions in text are bound to be hard to link
to DBPedia.
Coupled to this, Word ambiguities inherent to natural language like synonymy and homonymy (different surface forms referencing to the same entity and similar surface forms refering to different entities respectively) contributes to making this problem harder to solve \cite{probabilistic}.
Also, according to \cite{framework} current evaluation methods use non comparable metrics, and inconsistent terminology, which makes it hard to see 
areas of opportunity for research in this area.

This paper's practical contribution focuses on identifying and describing the cases on which the infrequency with which an entity is mentioned in text
leads to its incorrect linking,
and measuring the size of this set of cases; all of this applied on a big dataset of the newswire domain.
One main objective of the paper is to justify the discussion on describing the Long-tail entity set in order to advance research in this area.

\subsection{Infrastructure requirements for entity linking}
% Ways of using entity linkers at the moment. Use their REST service or deploy your own. 
% Memory requirements
%% How is this related: Difficult to conduct research without knowledge of infrastructure cluster management or a server that satisfies this requirements. 
%% How I build on this: Build a system that uses Cloud-based technologies for tagging data and querying the results. 
Given the statistical nature of the state-of-the art in Entity Linking, the models used by these algorithms are trained on massive amounts of data and therefore consume large amounts of memory.
This means that, in order to use them to tag text, high-end computing resources are needed.
As an example, DBPedia Spotlight, amongst the current state of the art in Entity Linking \cite{rizzo2012nerd},
requires at least 8 MB of data \cite{rw_elo_isem2013daiber} to load the language models and spots dictionaries required for tagging text.
This requirement of computing resources, means that the system often cannot be run in the researchers machine
and therefore requires some deployment to a server and using it as a server \cite{rw_elo_spotlightwebservice}.
This slows down research, since the management of these computing resources is time-consuming and expensive.

Since the comparison of entity linking services like DBPedia Spotlight and entity recognizers like Stanford's NER
is necessary in order to identify the size of the Long-tail entity dataset,
in order to provide a base for further research in this area, this paper also focuses on building a system architecture that leverages cloud technologies in order to minimize the impact in time and cost that running these services has on research. 

\todo{Show related work on long tail entities (cite the two references that you have)}
\subsection{NIL Entities versus Long-tail Entities}\label{nilvslongtail}
% How are NIL Entities described in current papers.
% What work is been done with them at the moment.
%% Justification of differentiating between NIL Entities and Long-tail Entities
%% Long-tail entities are specific to their domain. They must be identified isolated within their domain.
%% Commercial relevance of Long-tail entities suggestion (show commercial applications that do Long-tail entity recognition)
%% Describing and measuring the set of Long-tail entities allows businesses who benefit from IE tasks to decide if they will benefit from performing more granular IE tasks, which can be expensive (supervised learning of Long-tail entities requires labeling of data) to identify Long-tail entities.
Currently, there is some work been done on classification of unlinkable or NIL entities \cite{someone}.
\cite{someone} does this, while \cite{someone} does that.
However, both of these approaches fail to properly define Long-tail entities as their own type of entity, of which NIL entities are only a subset.
This papers gives a broader definition of the Long-tail Entity set from which further research can be derived. 

\subsection{The Signal 1M Dataset}
The Long-tail entity set is directly related to the distribution of the corpora it lies in.
Therefore, constraining this paper to a specific domain is necessary before been able to describe the set.

In this paper, the research of Long-tail entities has been constrained to the news domain;
to achieve this, the corpora chosen to do the analysis was the Signal Media One-Million News Articles Dataset\cite{Signal1M2016}.

The dataset, as described by \cite{Signal1M2016}, consists of 1 million articles mainly in English,
also including some articles translated from other languages.
Each article includes, beside its content, the following meta-data:
\begin{itemize}
  \item Unique global article identifier
  \item Title
  \item Media-type
  \item Source
  \item Published date
\end{itemize}

The general statistics about the dataset, show that articles have an average length of 405 words and come from over 93,000 different sources.
Even though the dataset is mainly composed of News Articles (over 70\%), it also contains Blog Articles\cite{Signal1M2016}.
However, the distribution of newswire data in the dataset is big enough for this research to be constrained to the News domain.
