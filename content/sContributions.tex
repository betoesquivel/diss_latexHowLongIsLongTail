\section{Describing the Long-tail entity set}
For any research to be done in Long-tail entities, a description that delimits the set is in order.
Long-tail entities, would be entities that are seldom mentioned in text and therefore form part of the "long-tail" of the distribution of entities.
However, this doesn't say much. 

This section will describe the exploratory process followed to give a first broad description of Long-tail entities and their characteristics.
It will then list some limitations of the process, and suggestions to further improve the description of the set of Long-tail entities.

\subsection{Manually exploring the data}
Working under the hypothesis that Long-tail entities are lost somewhere in the process between spotting candidates to link and actually linking them to a KB,
a small subset of 100 articles was randomly extracted from Signal's 1M Dataset, and tagged with three different taggers:
\begin{itemize}
\item Stanford Named Entity Recognizer\cite{rw_elo_finkel2005}
\item Wikifier (Named Entity Linker)\cite{wikifier_used}
\item Tagme (Named Entity Linker)\cite{tagme} \todo{cite wikifier and tagme!!!}
\end{itemize}
Afterwards, all tags per document where displayed in an application (source included in the appendixes), in order to visually see the differences between a Named Entity Reconition output, and a Linker's output.
It was hypothesised that Long-tail entities layed in the differences between the Linkers and Stanford's NER.
These figures show the output of the application for different documents, and how it showed the differences between taggers and linkers.


\subsection{Identification of types of Long-tail Entities}
A manual evaluation of the previous samples led to the following types of main Long-tail entities:
\begin{enumerate}
\item Entities whose mentions have no page on Wikipedia, and are therefore not linked
\item Entities whose mentions are not on Wikipedia, but are refered to by a surface form with an existing unrelated Wikipedia page.
\end{enumerate}
The second type of Long-tail entity, in the small sample explored in the previous section, usually occured on PERSON Enumex types.
Whereas the first type, happened mostly on Organizations.
Here is an example of both:

\todo Add an image to both types of mistakes

As can be seen in figures \ref{fig:mislinked_person} and \ref{fig:mislinked_organization}, there are two ways in which these types of mistakes manifest.
First, there is an overlap between the linker's and the NER tagger's tags. The linker links only a subset of the surface form, and links it with a different type.
On the other hand, the second type of long-tail entity manifests as a correct matching overlap between the surface form tagged by the the linker and the tagger.
However, the actual link is to an unrelated popular entity with an entry on the KB. 

Both of these ENUMEX types are of high priority for many NLP applications, and specifically within information retrieval. 

Long-tail entities explored in this paper will be limited to the first type identified, considering it should be enough to serve as a basis for further research,
and to show the importance of doing research in this area. 


\subsection{Proposal for measuring size of Long-tail Entity set}
The main Long-tail Entity types identified, led each to a proposal for identitying it in a corpus. 
For the first type, entities whose mentions have no page on Wikipedia, finding them would require the following algorithm:
\todo{SHOW AS ALGORITHM}\
\begin{enumerate}
\item Get an article
\item Tag the article with an NER tagger
\item Tag the article with an Entity Linker
\item Find all tags from the NER tagger, whose offsets do not exactly appear in the tags found by the Entity Linker
\end{enumerate}
This paper will explore the first type of Long-tail entities, by comparing the output of the state-of-the-art approach to NER from Stanford's NER Tagger
to Spotlight's DBPedia.

On the other hand, identifying and getting a list of the other type of Entity would involve some further disambiguation,
using the context around the mentions in order to semantically describe them and compare them to their link to see if there is a significant similarity.
This approach wasn't taken due to time limitations, and the focus of the paper was on identifying the first type of Long-tail entities.

\subsection{Limitations}
The nature of Long-tail entities is purely statistical, therefore any approach to describe it requires a large random sample that's specific to the
domain of study.
In this case, the large sample is used, however due to the soft max limit of concurrent Lambda functions running set by Amazon per region, the tagging process
was to slow to evaluate the entire dataset using the existing system. 
However, the limit can be increased by just submitting a support ticket to Amazon, and that would allow for a more appropriate level of parallelism to be used.

\section{Building a scalable cloud-based no-management Tagger Comparing System} \label{TaggerSystem}
% Describe what a tagger is
% List requirements of the system
% Justify selection of Amazon as the cloud infrastructure to use
In order to facilitate research on Long-tail entities using actual domain-specific data, a robust infrastructure is needed.
This section lists the requirements for a Tagger Comparing System decided on by the author based on the context of the paper.
After that, it describes the third-party off-the-shelf services used for the components of the system.
Following is a description of the built system's architecture and the main design decisions made during the build process.
Finally, the section concludes with a summary of the system's capabilities, and its main limitations. 

\subsection{Tagger Comparing System requirements}
In order to define the requirements for a system it's purpose and use case has to be laid out.
The main direct use of this system was for the tagging of a large number of articles with at least two different taggers (an NER Tagger and a linker) and getting statistics over the resulting tags.
This research is mainly exploratory, and therefore it must allow further modification of the components and the strategy to tag and compare the text.

Taking this use case into account, the following requirements for the system were specified:
\begin{enumerate}
\item Can store test data in a persistent storage available to all system components.
\item Minimizes new tagger deploy time.
\item Can run multiple tagging processes at the same time.
\item Indexes tags by:
  \begin{enumerate}
  \item Start character offset.
  \item Tagger
  \item Article ID
  \item ENUMEX type
  \end{enumerate}
\item Can index tags using other features if needed.
\item Can be reproduced with resources available to new researchers in this area.
\item Minimizes time spent managing infrastructure.
\end{enumerate}

The main architectural driver for the system, identified during the listing of its requirements, was the need for high modifiability and performance.
Typically, building a system that can satisfy these requirements from the ground up with good performance would require a high amount of computing resources. 
However, at the time of writing, the decision was made for building a system that could easily be reproduced by aspiring researchers in this area.

Therefore, the last requirement was added, which naturally let to the need for a normalized definition of "resources available" to new researchers.
Amazon provides a \$100 credit for educators and students on their platform, so this was used as the main resource budget constraint for the system.
Which means that the whole system must only be implemented using services running on the Amazon Web Services platform.

\subsection{Relevant AWS Services description}
\todo{Cite amazon services}
\todo{REMEMBER TO CITE FROM FRAMEWORK THE TYPE OF MISTAKES if you can}
Given the requirements and main resource budget constraint described above, the following Amazon Web Services where identified as necessary for the building of the system.
Also, after the Tagger System was run on the data, the number of tags obtained was to large for DynamoDB to be a practical querying interface to explore the data.
Therefore, AWS EMR\cite{aws_emr} was added to the AWS services stack for querying the whole dataset by bringing it into memory on a Spark cluster.
\begin{description}
\item[Article Storage, Amazon S3]
Amazon S3 is Amazon's solution for Storage of objects too large to store on a Database.
It provides buckets as the main containers for objects, which can be organized into a folder structure inside each bucket,
are replicated within a region to ensure high availability, and are accessed by a unique identifier or key.
All articles from the Signal's Dataset where stored in S3, since the only information used from the article was the content for the tagging.
Before uploading the articles to Amazon, they where tokenized into sentences.
This was done in order to provide any tagger with the flexibility of tagging an article in batches of sentences, since some entity linkers work better with short texts, like Tagme\cite{tagme}.
\item[Tag Storage, Dynamo DB]
Dynamod DB is advertised by Amazon as a no-management predictable-performance no-sql database. When setting up a table in the service, the estimated throughput (read and writes per second) is defined by the user, and Amazon allocates the resources necessary to provide that throughput. 
  
On DynamoDB, a user only pays for the resources allocated for tables hourly.
This allows for a fairly granular cost control when using the service.
For this research's particular case, high throughputs where only set on the Tags table when performing tagging jobs or when reading all of the data into S3 for exploration.
After this processes where completed, the throughput was lowered again preventing further charges to be applied for the table.

Each table in DynamoDB is schemaless to a point. 
This is because in order for DynamoDB to acomplish predictable performance, it needs to index data in a way that makes it easy to access concurrently, therfore, every record must be composed of either just a unique key, or a Hash key to sort the data and a Range key to uniquely identify the record within it's hash.
In this way, Dynamo can execute multiple writes at the same time in different partition nodes, assuming the access pattern of the data in the table is uniform.
This is the caveat of using Dynamo.
The performance is dependent on the access pattern of the data.
Therefore, Hash and Hash/Range keys must be selected in a way that predict how data is going to be accessed, otherwise, one partition of the data is going to be queried more times than the rest and DynamoDB won't  be able to leverage its distributed architecture.

Nonetheless, the batch nature of the task to be performed in this paper, allowed for the full control of the access patterns for the data on Dynamo,
so it was a perfect way of getting a high performance distributed store that could be accessed by multiple concurrent processses at the same time.
\item[Performance and Scalability, Amazon Lambda]
One of the core services for the developing of an economic, performant and scalable tagger comparing system is Amazon Lambda.
It allows for functions to to be written in either NodeJS, Java or Python and deployed on their infrastructure.
On creation of a function, the user only specifies the resources needed for the function to run, and optional triggers that will execute the function.
After this, the function can be called using the AWS SDK, and Amazon will scale up or down, the resources allocated for the execution of the function
depending on the load the function is receiving.
The user will only be charged for the amount of time the function is running, and the resources it consumed in that time.

So, specifically for this project, Amazon Lambdas, allowed for the definition of processes to either create tags or upload them to DynamoDB concurrently
without having to think about creating an elastic cluster with the computing resources necessary for the batch processing of tagging jobs. 

One caveat with this service is that there is a soft-limit of number of concurrent executions per second per region of 100.
This can be lifted with a simple request to the support centre in Amazon, however, it takes time to justify the lifting of the limit.
Because of this reason, this research's experiment only processed a sample of 100,000 articles out of the 1,000,000 in the used dataset.
All of the tagging job requests to Amazon Lambda functions coded for this project where been throttled so the amount of data processed had to be reduced.
\item[Component Inter-communication, Amazon Simple Queue Service] 
Amazon's Simple Queue Service is a reliable high-performance queuing service that works perfectly with highly distributed architectures.
It provides a reliable way to communicate jobs between services following a simple Producer and Consumer model. 

The pricing of this service is not high, and in the case of this research, given that the first 1,000,000 requests are free for starting users,
it was costless to use.

\item[Data exploration and analysis, Amazon EMR Cluster with Hive and Spark enabled] Description
This is one of the more expensive Amazon solutions used. There was no free-tier that fit the requirements for this project.
Amazon provides an already setup cluster, allowing the user only to tune specifications of the cluster like the number and type of the nodes.
It also gives the user the option to install popular applications for working with Big Data like: Hadooop, Hive, MapReduce, Pig, Spark, ...
This is the Amazon Service used that requires the most setup, and it is still straightforward and can all be done from Amazon's console. 

With just a few minutes of configuration, a user can have a full powerful cluster for performing data analysis, without having to spend too much 
time or money allocating or managing the infrastructure.
\end{description}


\subsection{Tagger Comparing System architecture}
After selecting the Amazon Web Services that suited the system's needs, the system architecture resulted in a distributed system where the main components where the AWS Lambda functions processing jobs.
These jobs where moved through a pipeline by dequeuing and queuing them into different queues from Amazon SQS.

Each full job processing step in the system is represented by a Process component composed of the following sub-components:
\begin{enumerate}
\item Input job (AWS SQS queue)
\item Input job queue listener and distributer (AWS lambda function)
\item Job consumer (AWS lambda function)
\item Output processed job storage (AWS DynamoDB or SQS)
\end{enumerate}
The queue job listener and distributer, does only that, it reads messages from only one queue, and calls a Job consumer to process them.

While the job consumer, must accept $n$ messages ($n > 0$), and for each message:
\begin{enumerate}
\item Process the message.
\item Move processed message into output queue or index in DynamoDB.
\item Delete the message from its originating queue.
\end{enumerate}

If a message is not processed correctly, then it must not be deleted from its originating queue.
Each message consumed from an Amazon SQS will be made available again for reading if it is not deleted after a period of time of it been read.
This timer is configured by the user upon creation of the queue and can be modified at any time.
For the purpose of this system, Amazon's SQS Dead Letter Queues mechanism was used.
Every job in a queue keeps a count of the number of times it has been read, and when that number reaches $20$ (specified by the user on creation as well), the job is moved to another queue for further inspection. This helps avoid the queue getting cluttered with unprocessable jobs. 

Finally, any common data that required no transformation and needed to be accessed in bulk only by key, was kept in an S3 bucket.
Using this project as an example, all articles from the Signal 1M dataset, tokenized into sentences,
where kept in JSON format within an S3 bucket using their id as key.

Following is a list of all AWS Lambda functions included in the system whose main job was processing data.
Their code, along with a full listing of the other functions used by the System
can be seen in section \ref{appendices}:
\todo{describe this functions}
\begin{description}
\item[StanfordNERTagger]
Java function that uses the CRFClassifier class from the stanford-corenlp main java package to tag raw text.
It receives as input a list of strings, and for each string, it returns a JSON representation of a list of maps.
Each map respresents a tag and contains the following key-value pairs:
\begin{itemize}
  \item Start offset: Integer
  \item End Offset: Integer
  \item Type: String
\end{itemize}
\item[DBTagUploader]
Python function that receives a list of tagged articles and uploads them to DynamoDB using the boto3 package
offered by Amazon to interface with Dynamo.
This function is also in charge of queuing tagging jobs for the SpotlightTagger function.

The algorithm followed by this function in pseudocode is as follows:
\input{algorithms/DBTagUploader.tex}


\item[SpotlightTagger]
NodeJS function that uses Spotlight's REST API to tag a group of 10 strings.
This function is also in charge of enqueuing the tagged article for DBUpload and dequeing the tagging job,
marking it as done.

\end{description}

 
\subsection{Tagger Comparing System summary}
% How to adding taggers to the system
% Amazon Lambda's soft 100 concurrent executions limit
In summary the resulting system provides the following main capabilities:
\begin{description}
\item[Scalability]
The system provides the user high granularity when selecting the number of processes to be run in parallel.
Each job distributer lambda function in the system decides how many consumer processes it starts each time it reads from it's corresponding input queue. 
So, for example, a user may choose to tag 1000 articles at a time by just having the job distributer read from the queue and call the consumer for 1000 jobs.
This is by far the most important requirement satisfied, given the resource budget constraint.
\item[Fast modifiability]
Changes to the underlying job consumer lambda functions require only the code to be updated and uploaded as a zip file to Amazon through their console.
Also, adding a new Process component to the pipeline requires only:
  \begin{itemize}
  \item
  Selecting a queue to consume jobs from, or creating and filling one with jobs if necessary.
  \item
  the deployment of the Job distributer and job consumer lambda functions.
  \item
  Selecting a queue or a table in DynamoDB to output the processed jobs to.
  \end{itemize}
\end{description}
All of these are provided by using only the AWS Services described above, which  minimizes the costs and infrastructure management time,
therefore satisfying the requirements previously stated. 

\section{Finding Long-tail Entities in the Signal's 1M Dataset}\label{Contrib:LongTailInSignal}
This section begins by showing the process of using the built Tagger Comparing System in (\ref{TaggerSystem}) to tag $100,000$ articles from Signal's 1M Dataset.
This will consist of explaining the decisions taken for tagger parameters and the tagging process level of concurrency.
After this, the system's performance will be described using the general metrics in AWS CloudWatch\cite{aws_cloudwatch}.
Once the tagging process is described, section the last section will focus on describing the process of querying the resulting tagged articles and the results of this process. 

\subsection{Using the tagger comparing system}\label{TaggingSignal}
Setting up the tagger comparing system consisted of a series of ETL steps to preprocess the Signal 1M Dataset, load it into Amazon S3, and add 
article id's to the TaggingJobs SQS queue to feed them into the pipeline.
The steps to achieve this are listed in detail below:
\begin{description}
\item[Preprocess the articles]
The preprocessing step consisted of tokenizing all of the Signal's 1M Dataset by tokenizing it into sentences.
This process was done on the researchers machine since tokenization is not a slow process.
The only care that had to be taken was to load all of the articles into memory in order to avoid blocking on I/O. 
The preprocessing was done with a node script that called the Stanford CoreNLP server running also in the host's machine.
The reason Node was used was that the computation was going to be done by the server so the only requirement needed for the script was that it had good
asynchronous performance, which is what NodeJS excels at.

The database used to load the articles into memory for multiple concurrent instances of the tokenizing job script to read from was redis\cite{redis}.\todo{cite redis}
All articles were kept there as a HashMap.

\item[Upload tokenized articles to S3]


\end{description}

\subsubsection{Choosing tagger parameters and level of Concurrency}
The tagging of articles with the DBPedia entity Linker was done using their exposed rest service.
It allows for the configuration of the confidence value used when determining whether a candidate link is a good match for the 
mention / surface form. 
In order to maximize link recall, the confidence value was set to a low value of $0.15$.

After setting this configuration option, the only other configuration parameter sent to the service was a filter of types to
retrieve. The filter was specified using the following value: \textit{"DBpedia:Person,DBpedia:Organization,DBpedia:Place"}.

After deploying the Lambda functions for each respective tagger to AWS Lambda.
The Node SQS tagging job consumers where deployed.
To select the concurrency level, a parameter was specified on the lambda function specifying how many invokations of the 
corresponding tagging Lambda functions were going to be executed at the same time.

\subsubsection{Tagger Comparing System performance}
In the end, the first preprocessing step, was not useful for the taggers.
The main purpose of doing the sentence tokenization was to allow taggers to tag articles using batches of sentences
in case their performance was better on small texts, like Tagme\cite{tagme}.
However, due to time limitations, only Spotlight DBPedia was used to tag the articles, and it's performance was good
on the full articles. 

\subsection{Type-1 Long-tail Entities in Signal's 1M Dataset} \label{ResultsSignal}
Once the sample of $100,000$ articles had been tagged by both DBPedia Spotlight and Stanford's NER Tagger,
over 3 million tags were stored in the system's database.
Querying this amount of data directly in DynamoDB would be an ardous process due to Dynamo's max bytes return limit per query.
Paginating over the results of every query would be slow and cumbersome.
Therefore, a more ideal solution to querying the data was chosen.

\subsubsection{Setting up an Amazon EMR Spark Cluster for querying the Tags table}
Amazon EMR Spark Cluster \todo{Describe Amazon EMR} was used as a platform to load all of the table in memory for querying.

Due to the size of the dataset, an Amazon EMR Spark Cluster with the following characteristics had to be setup:
\begin{itemize}
  \item 1 master node
  \item 2 core nodes
  \item 3 task nodes
  \item m3.xlarge instance type for all nodes
  \item Applications installed
    \begin{itemize}
      \item Hive
      \item Hue
      \item Spark
      \item Zeppelin
    \end{itemize}
\end{itemize}
The master node and core nodes where the default setup for a Spark Cluster, and where left that way,
but to increase the level of concurrency with which DynamoDB data was to be read, one extra task node was added.
\todo{parallelism relation to task nodes in spark cluster reference} 

After Amazon provisioned and started the nodes in the cluster, using the cluster was a matter of
\textit{ssh}-ing into it with the key pair set at cluster creation. 
Then, a \textit{Hive}\cite{hive} interpreter was started in the cluster and the following query was used to export the Tags table into S3
so that Spark could later load it into memory.
\input{queries/tags.sql}

\subsubsection{Querying the Tags table}
Once the \textit{Tags} table had been created, \textit{Zeppelin}\cite{zeppelin} was used to interact with the spark cluster to cache the table into memory with the following command.
\input{queries/cache_table.sql}

Caching the table allowed for the fast querying of the tag table, which fit with the exploratory quality of the research work to be done on the data.
The following queries where tested in \textit{Zeppelin}, and then persisted by inserting their output into an Amazon \textit{S3} bucket with \textit{Hive}.
\input{queries/tagger_tags.sql}
\input{queries/overlapping_tags.sql}
\input{queries/nil_tags.sql}

Following this process, the resulting set of tables was queried to find Long-tail entities of the first type defined in \ref{describinglongtail}. 
First, each table was grouped by ENAMEX type, in order to get statistics of the distribution of tags based on their type.
Here is an example query to achieve this for the Tags table.
\input{queries/group_by_type.sql}\todo{look for the query}

Next, the Overlapping tags table was split into two different sets based on the type agreement between the taggers used (Stanford NER and DBpedia Spotlight):
\begin{enumerate*}
  \item type agreement
  \item type disagreement
\end{enumerate*}
Below, is the example query used for describing type agreement by the taggers on overlaps:
\input{queries/overall_type_agreement.sql}\todo{look for the query}

Finally, combining the NIL and Overlps table with Stanfords table into one query, an overall description of Long-tail entities of type 1 using the differences
between Linked and Recognized Named Entities could be achieved using the following query:
\input{queries/overall_longtail_distribution.sql}\todo{look for the query}


\subsubsection{Results}
The previous queries were done over the resulting $N$\todo{how many tags?} tags found by the Tagger Comparing System (\ref{TaggerSystem}) and that led to the following results.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \label{fig:stanfordpertype}
        \centering
        \csvautotabular{data/stanford_per_type.csv}
        \caption{Stanford tags grouped by ENAMEX type}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \label{fig:spotlightpertype}
        \centering
        \csvreader[%
        respect all,%
        autotabular%
        ]{data/spotlight_per_type.csv}{}{\csvlinetotablerow}
        \caption{Spotlight tags grouped by ENAMEX type}
    \end{subfigure}
    \caption{Stanford and Spotlight tags grouped by ENAMEX type}
\end{figure*}


After the manual exploration done on the small 100 article sample in section (\ref{pending}),
the ENAMEX types ORGANIZATION and PERSON were identified as been the ones on which Stanford NER tags differed the most from entity Linkers.
Even though a different entity linker was used for the main comparison with Stanford's NER, from these results,
it can be infered that the same relationship is going to be kept between the NER and the linker.

Stanford's predominant ENAMEX tag type was PERSON, while Spotlight's predominant tag type was LOCATION.
Naturally, the NER found more tags in the article.
So far, this could be considered as support for the established hypothesis\todo{Establish the hypothesis} that Long-tail entities lay mainly in the differences between 
the mentioned and linked entities in an article.
Meaning that the type of entities for which there is a link to an entity of the same type as the mention's ENAMEX type, but that refers to another entity
are less common, and don't have as big of an effect on commercial produts as type-1 long-tail entities.\todo{establish types of entities properly}
However, further exploration of the long-tail entities of this type was needed in order to confirm this assumption.

\begin{figure}
  \label{fig:nilspertype}
  \centering
  \csvautotabular{data/nil_per_type.csv}
  \caption{NIL entities per type}
\end{figure}

On the other hand, in the NIL entity mentions table, all the mentions found by the NER whose offsets did not overlap the linked entities offsets,
the distribution was mainly skewed towards the ORGANIZATION ENAMEX type.
Almost half of the ORGANIZATIONS tagged by the NER tagger where not tagged by the linker.
Differently, PERSONS and LOCATIONS where less commonly missed by the Entity Linker.
It is important to note, that within the PERSON type tags that where found by Spotlight and Stanford NER, there could be a subset of tags which are linked to unrelated entities,
that this analysis doesn't find.

Even so, further exploration was done on the overlapping tags found by both taggers,
in order to find what was the agreement between the taggers on ENAMEX type of tags. 
This led to the following data:

\begin{figure}
  \label{fig:avgagreementpercentnernel}
  \centering
  \csvautotabular{data/average_percent_ner_nel_agreement_per_article_in_overlapping_tags.csv}
  \caption{Average percent agreement per article between StanfordNER and Spotlight}
\end{figure}

When grouping all the overlaps per article, the average percentage type disagreement between the Linker and the NER was of 22.5\%.
This means it is likely that the taggers will assign the same type to a given tag, once they both found it.
Nevertheless, the disagreement percent is still significant and worth exploring.

\begin{figure}
    \centering
    \begin{subfigure}
        \label{fig:disagreementpertype}
        \centering
        \csvautotabular{data/ner_nel_disagreement_on_overlapping_tags_per_type.csv}
        \caption{Disagreement on overlapping tags per type}
    \end{subfigure}%
    \begin{subfigure}
        \label{fig:agreementvsdisagreementpercent}
        \centering
        \csvautotabular{data/overall_overlap_type_matches_vs_mismatches_percent.csv}
        \caption{Overall overlapped tags agreement between StanfordNER and Spotlight}
    \end{subfigure}%
    \begin{subfigure}
        \label{fig:agreementnernelbytype}
        \centering
        \csvautotabular{data/ner_nel_agreement_in_overlapping__by_type_matches_vs_mismatches_per_type_percent.csv}
        \caption{Overall overlapped tags agreement between StanfordNER and Spotlight by type}
    \end{subfigure}%
    \caption{ENAMEX type agreement on overlaps between StanfordNER an Spotlight}
\end{figure}

As can be seen in the tables above, the overlapping ENAMEX type agreement between the NER tagger and the linker occured exclusively on
the PERSON and LOCATION types in the documents tagged.
On the overlapping tags, all ORGANISATIONS found by the NER tagger were typed differently by the linker.
It is safe to assume that, when it comes to LOCATION ENAMEX types, both Named Entity recognition and linking have similar performance,
and the Stanford NER tagger didn't use any gazzeteers\todo{stanford gazeteers???}\cite{stanfordfaq} for the tagging process.

\begin{figure}
    \centering
    \begin{subfigure}
        \label{fig:longtailpertype}
        \centering
        \csvautotabular{data/longtail_per_type.csv}
        \caption{Long-tail entities per ENAMEX type}
    \end{subfigure}%
    \begin{subfigure}
        \label{fig:longtail}
        \centering
        \csvautotabular{data/overall_longtail.csv}
        \caption{Overall Long-tail tags}
    \end{subfigure}%
    \begin{subfigure}
        \label{fig:avglongtailpercentperarticle}
        \centering
        \csvautotabular{data/average_longtail_percent_per_article.csv}
        \caption{Average Long-tail tag percentage per article}
    \end{subfigure}%
    \caption{Describing Long-tail entities in 100,000 tagged articles using differences between StanfordNER and Spotlight taggers}
\end{figure}

Next, combining the NIL table with the overlapping tags for which there is type disagreement, a broad description of the Long-tail entity set for the dataset can be given.
The distribution of long-tail entities on the corpus per type, can be seen on the next table.
Showing again how the ENAMEX ORGANIZATION type is the most common.
The overall distribution of long-tail entity mentions them consits of 11\% of all the Stanford NER mentions.
Again, an interesting analysis, consists of grouping the long-tail entities per article and averaging their distributions per article.
This gives a surprising average of over 24.35\%.
The difference between this value and the overall Long-tail distribution could mean that there are some articles predominantly conformed of long-tail entities.

\todo{add sub conclusion to all data evaluations}
