\section{Describing the Long-tail entity set}
For any research to be done in Long-tail entities, a description that delimits the set is in order.
Long-tail entities, would be entities that are seldom mentioned in text and therefore form part of the "long-tail" of the distribution of entities.
However, this doesn't say much. 

This section will describe the exploratory process followed to give a first broad description of Long-tail entities and their characteristics.
It will then list some limitations of the process, and suggestions to further improve the description of the set of Long-tail entities.

\subsection{Manually exploring the data}
Working under the hypothesis that Long-tail entities are lost somewhere in the process between spotting candidates to link and actually linking them to a KB,
a small subset of 100 articles was randomly extracted from Signal's 1M Dataset, and tagged with three different taggers:
\begin{itemize}
\item Stanford Named Entity Recognizer\cite{rw_elo_finkel2005}
\item Wikifier (Named Entity Linker)\cite{wikifier_used}
\item Tagme (Named Entity Linker)\cite{tagme}
\end{itemize}
Afterwards, all tags per document where displayed in an application (source included in the appendixes), in order to visually see the differences between a Named Entity Reconition output, and a Linker's output.
It was hypothesised that Long-tail entities layed in the differences between the Linkers and Stanford's NER.
These figures show the output of the application for different documents, and how it showed the differences between taggers and linkers.


\subsection{Identification of types of Long-tail Entities}
A manual evaluation of the previous samples led to the following types of main Long-tail entities:
\begin{enumerate}
\item Entities whose mentions have no page on Wikipedia, and are therefore not linked
\item Entities whose mentions are not on Wikipedia, but are refered to by a surface form with an existing unrelated Wikipedia page.
\end{enumerate}
The second type of Long-tail entity, in the small sample explored in the previous section, usually occured on PERSON Enumex types.
Whereas the first type, happened mostly on Organizations.
Here is an example of both:
\todo Add an image to both types of mistakes

As can be seen in figures \ref{fig:mislinked_person} and \ref{fig:mislinked_organization}, there are two ways in which these types of mistakes manifest.
First, there is an overlap between the linker's and the NER tagger's tags. The linker links only a subset of the surface form, and links it with a different type.
On the other hand, the second type of long-tail entity manifests as a correct matching overlap between the surface form tagged by the the linker and the tagger.
However, the actual link is to an unrelated popular entity with an entry on the KB. 

Both of these ENUMEX types are of high priority for many NLP applications, and specifically within information retrieval. 
\todo add the statistics on queries for information retrieval, somewhere

Long-tail entities explored in this paper will be limited to the first type identified, considering it should be enough to serve as a basis for further research,
and to show the importance of doing research in this area. 


\subsection{Proposal for measuring size of Long-tail Entity set}
The main Long-tail Entity types identified, led each to a proposal for identitying it in a corpus. 
For the first type, entities whose mentions have no page on Wikipedia, finding them would require the following algorithm:
\todo{HOW DO I SHOW AN ALGORITHM?}\
\begin{enumerate}
\item Get an article
\item Tag the article with an NER tagger
\item Tag the article with an Entity Linker
\item Find all tags from the NER tagger, whose offsets do not exactly appear in the tags found by the Entity Linker
\end{enumerate}
This paper will explore the first type of Long-tail entities, by comparing the output of the state-of-the-art approach to NER from Stanford's NER Tagger
to Spotlight's DBPedia.

On the other hand, identifying and getting a list of the other type of Entity would involve some further disambiguation,
using the context around the mentions in order to semantically describe them and compare them to their link to see if there is a significant similarity.
This approach wasn't taken due to time limitations, and the focus of the paper was on identifying the first type of Long-tail entities.

\subsection{Limitations}
The nature of Long-tail entities is purely statistical, therefore any approach to describe it requires a large random sample that's specific to the
domain of study.
In this case, the large sample is used, however due to the soft max limit of concurrent Lambda functions running set by Amazon per region, the tagging process
was to slow to evaluate the entire dataset using the existing system. 
However, the limit can be increased by just submitting a support ticket to Amazon, and that would allow for a more appropriate level of parallelism to be used.

\section{Building a scalable cloud-based no-management Tagger Comparing System} \label{TaggerSystem}
% Describe what a tagger is
% List requirements of the system
% Justify selection of Amazon as the cloud infrastructure to use
In order to facilitate research on Long-tail entities using actual domain-specific data, a robust infrastructure is needed.
This section lists the requirements for a Tagger Comparing System decided on by the author based on the context of the paper.
After that, it describes the third-party off-the-shelf services used for the components of the system.
Following is a description of the built system's architecture and the main design decisions made during the build process.
Finally, the section concludes with a summary of the system's capabilities, and its main limitations. 

\subsection{Tagger Comparing System requirements}
In order to define the requirements for a system it's purpose and use case has to be laid out.
The main direct use of this system was for the tagging of a large number of articles with at least two different taggers (an NER Tagger and a linker) and getting statistics over the resulting tags.
This research is mainly exploratory, and therefore it must allow further modification of the components and the strategy to tag and compare the text.

Taking this use case into account, the following requirements for the system were specified:
\begin{enumerate}
\item Can store test data in a persistent storage available to all system components.
\item Minimizes new tagger deploy time.
\item Can run multiple tagging processes at the same time.
\item Indexes tags by:
  \begin{enumerate}
  \item Start character offset.
  \item Tagger
  \item Article ID
  \item ENUMEX type
  \end{enumerate}
\item Can index tags using other features if needed.
\item Can be reproduced with resources available to new researchers in this area.
\item Minimizes time spent managing infrastructure.
\end{enumerate}

The main architectural driver for the system, identified during the listing of its requirements, was the need for high modifiability and performance.
Typically, building a system that can satisfy these requirements from the ground up with good performance would require a high amount of computing resources. 
However, at the time of writing, the decision was made for building a system that could easily be reproduced by aspiring researchers in this area.

Therefore, the last requirement was added, which naturally let to the need for a normalized definition of "resources available" to new researchers.
Amazon provides a \$100 credit for educators and students on their platform, so this was used as the main resource budget constraint for the system.
Which means that the whole system must only be implemented using services running on the Amazon Web Services platform.

\subsection{Relevant AWS Services description}
Given the requirements and main resource budget constraint described above, the following Amazon Web Services where identified as necessary for the building of the system.
\begin{description}
\item[Article Storage, Amazon S3] Description
\item[Tag Storage, Dynamo DB] Description
\item[Performance and Scalability, Amazon Lambda] Description
\item[Component Inter-communication, Amazon Simple Queue Service] Description
\end{description}


\subsection{Tagger Comparing System architecture}
After selecting the Amazon Web Services that suited the system's needs, the system architecture resulted in a distributed system where the main components where the AWS Lambda functions processing jobs.
These jobs where moved through a pipeline by dequeuing and queuing them into different queues from Amazon SQS.

Each full job processing step in the system is represented by a Process component composed of the following sub-components:
\begin{enumerate}
\item Input job (AWS SQS queue)
\item Input job queue listener and distributer (AWS lambda function)
\item Job consumer (AWS lambda function)
\item Output processed job storage (AWS DynamoDB or SQS)
\end{enumerate}
The queue job listener and distributer, does only that, it reads messages from only one queue, and calls a Job consumer to process them.

While the job consumer, must accept $n$ messages ($n > 0$), and for each message:
\begin{enumerate}
\item Process the message.
\item Move processed message into output queue or index in DynamoDB.
\item Delete the message from its originating queue.
\end{enumerate}

If a message is not processed correctly, then it must not be deleted from its originating queue.
Each message consumed from an Amazon SQS will be made available again for reading if it is not deleted after a period of time of it been read.
This timer is configured by the user upon creation of the queue and can be modified at any time.
For the purpose of this system, Amazon's SQS Dead Letter Queues mechanism was used.
Every job in a queue keeps a count of the number of times it has been read, and when that number reaches $20$ (specified by the user on creation as well), the job is moved to another queue for further inspection. This helps avoid the queue getting cluttered with unprocessable jobs. 

Finally, any common data that required no transformation and needed to be accessed in bulk only by key, was kept in an S3 bucket.
Using this project as an example, all articles from the Signal 1M dataset, tokenized into sentences, where kept in JSON format within an S3 bucket using their id as key.

Following is a list of all AWS Lambda functions included in the system, and the queues they interacted with:
\begin{description}
\item[StanfordNERTagger] Description
\item[Tag Storage, Dynamo DB] Description
\item[Performance and Scalability, Amazon Lambda] Description
\item[Component Inter-communication, Amazon Simple Queue Service] Description
\end{description}

 
\subsection{Tagger Comparing System summary}
% How to adding taggers to the system
% Amazon Lambda's soft 100 concurrent executions limit
In summary the resulting system provides the following main capabilities:
\begin{description}
\item[Scalability]
The system provides the user high granularity when selecting the number of processes to be run in parallel.
Each job distributer lambda function in the system decides how many consumer processes it starts each time it reads from it's corresponding input queue. 
So, for example, a user may choose to tag 1000 articles at a time by just having the job distributer read from the queue and call the consumer for 1000 jobs.
This is by far the most important requirement satisfied, given the resource budget constraint.
\item[Fast modifiability]
Changes to the underlying job consumer lambda functions require only the code to be updated and uploaded as a zip file to Amazon through their console.
Also, adding a new Process component to the pipeline requires only:
  \begin{itemize}
  \item
  Selecting a queue to consume jobs from, or creating and filling one with jobs if necessary.
  \item
  the deployment of the Job distributer and job consumer lambda functions.
  \item
  Selecting a queue or a table in DynamoDB to output the processed jobs to.
  \end{itemize}
\end{description}
All of these are provided by using only the AWS Services described above, which  minimizes the costs and infrastructure management time,
therefore satisfying the requirements previously stated. 

\section{Finding Long-tail Entities in the Signal's 1M Dataset}\label{Contrib:LongTailInSignal}
This section begins with section \ref{TaggingSignal} showing the process of using the built Tagger Comparing System (\ref{TaggerSystem}) to tag $100,000$ articles from Signal's 1M Dataset.
This will consist of explaining the decisions taken for tagger parameters and the tagging process level of concurrency.
After this, the system's performance will be described using the general metrics in AWS CloudWatch\cite{aws_cloudwatch}.
Once the tagging process is described, section \ref{ResultsSignal} will focus on describing the process of querying the resulting tagged articles and the results of this process. 

\subsection{Using the tagger comparing system}\label{TaggingSignal}

\subsubsection{Choosing tagger parameters}

\subsubsection{Choosing level of concurrency}

\subsubsection{Tagger Comparing System performance}

\subsection{Type-1 Long-tail Entities in Signal's 1M Dataset} \label{ResultsSignal}
Once the sample of $100,000$ articles had been tagged by both DBPedia Spotlight and Stanford's NER Tagger,
over 3 million tags were stored in the system's database.
Querying this amount of data directly in DynamoDB would be an ardous process due to Dynamo's max bytes return limit per query.
Paginating over the results of every query would be slow and cumbersome.
Therefore, a more ideal solution to querying the data was chosen.

\subsubsection{Setting up an Amazon EMR Spark Cluster for querying the Tags table}
Amazon EMR Spark Cluster \todo{Describe Amazon EMR} was used as a platform to load all of the table in memory for querying.

Due to the size of the dataset, an Amazon EMR Spark Cluster with the following characteristics had to be setup:
\begin{itemize}
  \item 1 master node
  \item 2 core nodes
  \item 3 task nodes
  \item m3.xlarge instance type for all nodes
  \item Applications installed
    \begin{itemize}
      \item Hive
      \item Hue
      \item Spark
      \item Zeppelin
    \end{itemize}
\end{itemize}
The master node and core nodes where the default setup for a Spark Cluster, and where left that way,
but to increase the level of concurrency with which DynamoDB data was to be read, one extra task node was added.
\todo{parallelism relation to task nodes in spark cluster reference} 

After Amazon provisioned and started the nodes in the cluster, using the cluster was a matter of
\textit{ssh}-ing into it with the key pair set at cluster creation. 
Then, a \textit{Hive}\cite{hive} interpreter was started in the cluster and the following query was used to export the Tags table into S3
so that Spark could later load it into memory.
\input{queries/tags.sql}

\subsubsection{Querying the Tags table}
Once the \textit{Tags} table had been created, \textit{Zeppelin}\cite{zeppelin} was used to interact with the spark cluster to cache the table into memory with the following command.
\input{queries/cache_table.sql}

Caching the table allowed for the fast querying of the tag table, which fit with the exploratory quality of the research work to be done on the data.
The following queries where tested in \textit{Zeppelin}, and then persisted by inserting their output into an Amazon \textit{S3} bucket with \textit{Hive}.
\input{queries/tagger_tags.sql}
\input{queries/overlapping_tags.sql}
\input{queries/nil_tags.sql}

Following this process, the resulting set of tables was queried to find Long-tail entities of the first type defined in \ref{describinglongtail}. 
First, each table was grouped by ENAMEX type, in order to get statistics of the distribution of tags based on their type.
Here is an example query to achieve this for the Tags table.
\input{queries/group_by_type.sql}\todo{look for the query}

Next, the Overlapping tags table was split into two different sets based on the type agreement between the taggers used (Stanford NER and DBpedia Spotlight):
\begin{enumerate*}
  \item type agreement
  \item type disagreement
\end{enumerate*}
Below, is the example query used for describing type agreement by the taggers on overlaps:
\input{queries/overall_type_agreement.sql}\todo{look for the query}

Finally, combining the NIL and Overlps table with Stanfords table into one query, an overall description of Long-tail entities of type 1 using the differences
between Linked and Recognized Named Entities could be achieved using the following query:
\input{queries/overall_longtail_distribution.sql}\todo{look for the query}


\subsubsection{Results}
The previous queries, where done over the resulting $N$\todo{how many tags?} found by the Tagger Comparing System (\ref{TaggerSystem}) and that led to the following results.

\csvautotabular{data/stanford_per_type.csv}

\csvreader[%
   respect all,%
   autotabular%
 ]{spotlight_per_type.csv}{}{\csvlinetotablerow}%

\csvautotabular{data/nil_per_type.csv}

\csvautotabular{average_longtail_percent_per_article.csv}

\csvautotabular{data/average_percent_ner_nel_agreement_per_article_in_overlapping_tags.csv}
\csvautotabular{data/ner_nel_agreement_on_overlapping_tags_per_type.csv}
\csvautotabular{data/ner_nel_disagreement_on_overlapping_tags_per_type.csv}
\csvautotabular{data/ner_nel_agreement_in_overlapping__by_type_matches_vs_mismatches_per_type_percent.csv}

\csvautotabular{data/longtail_per_type.csv}
\csvautotabular{data/overall_longtail.csv}
\csvautotabular{data/overall_overlap_type_matches_vs_mismatches_percent.csv}
\csvautotabular{data/average_longtail_percent_per_article.csv}
